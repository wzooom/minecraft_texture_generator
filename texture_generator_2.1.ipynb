{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1S26lE1_XmXWcsiMVo_v0winoWPSnsrzD",
      "authorship_tag": "ABX9TyNu0fviA4DasIcpBaU64ipZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wzooom/minecraft_texture_generator/blob/main/texture_generator_2.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Reshape, Embedding, LSTM, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def preprocess_description(description):\n",
        "    \"\"\"Convert textual description into a numerical feature vector.\"\"\"\n",
        "    # Placeholder: Use pretrained embeddings like Word2Vec, GloVe, or BERT in a real implementation.\n",
        "    return np.random.rand(128)\n",
        "\n",
        "def build_texture_model(input_dim):\n",
        "    \"\"\"Build a neural network to generate 16x16 textures.\"\"\"\n",
        "    model = Sequential([\n",
        "        Dense(256, activation='relu', input_dim=input_dim),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(16 * 16 * 3, activation='sigmoid'),  # Output a 16x16 RGB texture\n",
        "        Reshape((16, 16, 3))\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "    return model\n",
        "\n",
        "def preprocess_filename(filename):\n",
        "    \"\"\"Generate a description based on a PNG filename.\"\"\"\n",
        "    return filename.replace(\".png\", \"\").replace(\"_\", \" \")\n",
        "\n",
        "def generate_png_dataset_from_drive(drive_path):\n",
        "    \"\"\"Process PNG files from Google Drive, including those in subfolders.\"\"\"\n",
        "    descriptions = []\n",
        "    texture_paths = []\n",
        "\n",
        "    # Walk through all folders and subfolders within the drive_path\n",
        "    for root, dirs, files in os.walk(drive_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.png'):\n",
        "                # Preprocess the filename to generate a description\n",
        "                descriptions.append(preprocess_filename(file))\n",
        "                texture_paths.append(os.path.join(root, file))\n",
        "\n",
        "    # Load textures (resize to 16x16)\n",
        "    textures = []\n",
        "    for path in texture_paths:\n",
        "        img = Image.open(path).convert('RGB').resize((16, 16))  # Resize to 16x16\n",
        "        textures.append(np.array(img) / 255.0)  # Normalize to [0, 1]\n",
        "\n",
        "    textures = np.array(textures)\n",
        "    return descriptions, textures, texture_paths\n",
        "\n",
        "def combine_datasets(dtd_features, dtd_textures, png_features, png_textures):\n",
        "    \"\"\"Combine the DTD and PNG datasets.\"\"\"\n",
        "    combined_features = np.concatenate((dtd_features, png_features), axis=0)\n",
        "    combined_textures = np.concatenate((dtd_textures, png_textures), axis=0)\n",
        "    return combined_features, combined_textures\n",
        "\n",
        "# Load DTD dataset\n",
        "def download_and_prepare_dtd():\n",
        "    \"\"\"Download and prepare the Describable Textures Dataset (DTD).\"\"\"\n",
        "    url = \"https://www.robots.ox.ac.uk/~vgg/data/dtd/download/dtd-r1.0.1.tar.gz\"\n",
        "    dataset_dir = \"dtd_dataset\"\n",
        "    tar_file = \"dtd.tar.gz\"\n",
        "\n",
        "    # Download DTD if not already downloaded\n",
        "    if not os.path.exists(tar_file):\n",
        "        print(\"Downloading DTD...\")\n",
        "        urllib.request.urlretrieve(url, tar_file)\n",
        "\n",
        "    # Extract DTD if not already extracted\n",
        "    if not os.path.exists(dataset_dir):\n",
        "        print(\"Extracting DTD...\")\n",
        "        with tarfile.open(tar_file, 'r:gz') as tar_ref:\n",
        "            tar_ref.extractall(path=\".\")\n",
        "\n",
        "    # Load texture paths and descriptions\n",
        "    texture_dir = os.path.join(\"dtd\", \"images\")\n",
        "    categories = os.listdir(texture_dir)\n",
        "    descriptions, texture_paths = [], []\n",
        "\n",
        "    for category in categories:\n",
        "        category_dir = os.path.join(texture_dir, category)\n",
        "        if os.path.isdir(category_dir):\n",
        "            for file_name in os.listdir(category_dir):\n",
        "                if file_name.endswith(('.jpg', '.png', '.jpeg')):\n",
        "                    texture_paths.append(os.path.join(category_dir, file_name))\n",
        "                    descriptions.append(f\"A texture that is {category}.\")\n",
        "\n",
        "    return descriptions, texture_paths\n",
        "\n",
        "def load_dtd_dataset():\n",
        "    \"\"\"Load DTD dataset and preprocess it.\"\"\"\n",
        "    descriptions, texture_paths = download_and_prepare_dtd()\n",
        "    features = np.array([preprocess_description(desc) for desc in descriptions])\n",
        "\n",
        "    textures = []\n",
        "    for path in texture_paths:\n",
        "        img = Image.open(path).convert('RGB').resize((16, 16))  # Resize to 16x16\n",
        "        textures.append(np.array(img) / 255.0)  # Normalize to [0, 1]\n",
        "\n",
        "    textures = np.array(textures)\n",
        "    return features, textures\n",
        "\n",
        "def split_dataset(features, textures):\n",
        "    \"\"\"Split dataset into training, validation, and testing sets.\"\"\"\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(features, textures, test_size=0.4, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Load Minecraft textures from Google Drive\n",
        "drive_path = \"/content/drive/My Drive/texture_generator\"\n",
        "png_descriptions, png_textures, png_files = generate_png_dataset_from_drive(drive_path)\n",
        "\n",
        "# Preprocess descriptions and create features for Minecraft textures\n",
        "png_features = np.array([preprocess_description(desc) for desc in png_descriptions])\n",
        "\n",
        "# Load DTD dataset\n",
        "dtd_features, dtd_textures = load_dtd_dataset()\n",
        "\n",
        "# Combine DTD and Minecraft datasets\n",
        "combined_features, combined_textures = combine_datasets(dtd_features, dtd_textures, png_features, png_textures)\n",
        "\n",
        "# Split the combined dataset\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(combined_features, combined_textures)\n",
        "\n",
        "# Build the texture generation model\n",
        "input_dim = 128\n",
        "model = build_texture_model(input_dim)\n",
        "\n",
        "# Train the model with the combined dataset\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "\n",
        "def generate_texture(description):\n",
        "    \"\"\"Generate a texture based on a description.\"\"\"\n",
        "    processed_description = preprocess_description(description)\n",
        "    processed_description = np.expand_dims(processed_description, axis=0)  # Add batch dimension\n",
        "    texture = model.predict(processed_description)[0]\n",
        "    return texture\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YNmdZfW2mu3",
        "outputId": "259c0d2a-c173-4ce1-d0b4-076723fde9a7",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Extracting DTD...\n",
            "Epoch 1/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.0687 - val_loss: 0.0684\n",
            "Epoch 2/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0675 - val_loss: 0.0685\n",
            "Epoch 3/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0666 - val_loss: 0.0687\n",
            "Epoch 4/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0685 - val_loss: 0.0699\n",
            "Epoch 5/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0674 - val_loss: 0.0688\n",
            "Epoch 6/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0670 - val_loss: 0.0692\n",
            "Epoch 7/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0672 - val_loss: 0.0720\n",
            "Epoch 8/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0665 - val_loss: 0.0696\n",
            "Epoch 9/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0658 - val_loss: 0.0696\n",
            "Epoch 10/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0661 - val_loss: 0.0711\n",
            "Epoch 11/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0634 - val_loss: 0.0705\n",
            "Epoch 12/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0633 - val_loss: 0.0731\n",
            "Epoch 13/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0629 - val_loss: 0.0718\n",
            "Epoch 14/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0620 - val_loss: 0.0739\n",
            "Epoch 15/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0613 - val_loss: 0.0751\n",
            "Epoch 16/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0608 - val_loss: 0.0735\n",
            "Epoch 17/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0589 - val_loss: 0.0751\n",
            "Epoch 18/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0576 - val_loss: 0.0797\n",
            "Epoch 19/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0560 - val_loss: 0.0789\n",
            "Epoch 20/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0532 - val_loss: 0.0834\n",
            "Epoch 21/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0542 - val_loss: 0.0823\n",
            "Epoch 22/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0519 - val_loss: 0.0819\n",
            "Epoch 23/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0506 - val_loss: 0.0857\n",
            "Epoch 24/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0512 - val_loss: 0.0845\n",
            "Epoch 25/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0479 - val_loss: 0.0837\n",
            "Epoch 26/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0469 - val_loss: 0.0865\n",
            "Epoch 27/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0475 - val_loss: 0.0871\n",
            "Epoch 28/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0448 - val_loss: 0.0883\n",
            "Epoch 29/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0432 - val_loss: 0.0891\n",
            "Epoch 30/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0436 - val_loss: 0.0893\n",
            "Epoch 31/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0420 - val_loss: 0.0906\n",
            "Epoch 32/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0416 - val_loss: 0.0938\n",
            "Epoch 33/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0405 - val_loss: 0.0903\n",
            "Epoch 34/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 0.0394 - val_loss: 0.0916\n",
            "Epoch 35/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0385 - val_loss: 0.0926\n",
            "Epoch 36/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0374 - val_loss: 0.0920\n",
            "Epoch 37/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0374 - val_loss: 0.0950\n",
            "Epoch 38/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0364 - val_loss: 0.0944\n",
            "Epoch 39/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0361 - val_loss: 0.0944\n",
            "Epoch 40/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0341 - val_loss: 0.0968\n",
            "Epoch 41/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0347 - val_loss: 0.0983\n",
            "Epoch 42/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0339 - val_loss: 0.1006\n",
            "Epoch 43/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0323 - val_loss: 0.0977\n",
            "Epoch 44/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0323 - val_loss: 0.0988\n",
            "Epoch 45/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0315 - val_loss: 0.1001\n",
            "Epoch 46/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0308 - val_loss: 0.1004\n",
            "Epoch 47/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0310 - val_loss: 0.1015\n",
            "Epoch 48/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0300 - val_loss: 0.1013\n",
            "Epoch 49/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0292 - val_loss: 0.1016\n",
            "Epoch 50/50\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0292 - val_loss: 0.0998\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0973\n",
            "Test Loss: 0.0982728824019432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "description = \"Rough beach sand.\"\n",
        "generated_texture = generate_texture(description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wivtAJZ-OtG7",
        "outputId": "97ce6b9b-df56-457c-fc1d-0650b97129c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7840126ae0c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the generated texture\n",
        "plt.imshow(generated_texture)\n",
        "plt.title(description)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "ob_X3VLcOoBJ",
        "outputId": "64c66cc2-b9ed-419d-c603-de968a120457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}